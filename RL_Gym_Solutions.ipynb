{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcc_DHQRc7f8"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install box2d-py\n",
        "!pip install gym[box2d]\n",
        "!pip install ma_gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBLTNvzNa6EZ",
        "outputId": "ca128eb4-fee5-4ac2-c842-d1dd46c95275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.7/dist-packages (0.19.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.3.0)\n",
            "Requirement already satisfied: pyglet>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet>=1.4.0->gym[box2d]) (0.16.0)\n",
            "Requirement already satisfied: ma_gym in /usr/local/lib/python3.7/dist-packages (0.0.8)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from ma_gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from ma_gym) (1.21.6)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ma_gym) (1.16.0)\n",
            "Requirement already satisfied: gym==0.19.0 in /usr/local/lib/python3.7/dist-packages (from ma_gym) (0.19.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from ma_gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from ma_gym) (1.3.0)\n",
            "Requirement already satisfied: pillow>=7.2.0 in /usr/local/lib/python3.7/dist-packages (from ma_gym) (9.1.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->ma_gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXJiX_uvc2pW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "from gym import spaces\n",
        "from google.colab import widgets\n",
        "import time\n",
        "from mpl_toolkits import mplot3d\n",
        "import copy\n",
        "import random\n",
        "import gym"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AC - Acrobot ðŸŒŽ"
      ],
      "metadata": {
        "id": "anST7uuoak6B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n45u14Ibak6J"
      },
      "source": [
        "## NN Definition (Our Q-Approximation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9brJH0F9ak6J"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(6, 100)\n",
        "        self.fc2 = nn.Linear(100, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmE2smvPak6J"
      },
      "source": [
        "## NN Definition (Actor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7dypK58ak6J"
      },
      "outputs": [],
      "source": [
        "class ActorNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(6, 200)\n",
        "        self.fc2 = nn.Linear(200, 100)\n",
        "        self.fc3 = nn.Linear(100, 50)\n",
        "        self.fc4 = nn.Linear(50, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.softmax(self.fc4(x), dim = -1)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JxSeG30ak6J"
      },
      "source": [
        "## Actor Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAMnR34Iak6J"
      },
      "outputs": [],
      "source": [
        "#Agent that uses the greedy epsilon approach\n",
        "class ActorAgent():\n",
        "\n",
        "  def __init__(self, Actor, action_space):\n",
        "    self.Actor = Actor\n",
        "    self.action_space = action_space\n",
        "\n",
        "  def get_action(self, state):\n",
        "      #return int(torch.argmax(self.Actor(state)))\n",
        "      t = self.Actor(state)\n",
        "      return int(random.choices([i for i in range(len(t))], [float(i) for i in list(t)])[0])\n",
        "      #return int(random.choices([0,1,2,3], [float(i) for i in list(t)])[0])\n",
        "\n",
        "  def __str__(self):\n",
        "    return f'This agent works in an environment with {len(self.Actor)} states and {self.action_space.n} possible actions'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HuFLmSRak6K"
      },
      "source": [
        "## Greedy-EPS Agent "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suhB7bgBak6K"
      },
      "outputs": [],
      "source": [
        "#Agent that uses the greedy epsilon approach\n",
        "class GreedyEpsAgent():\n",
        "\n",
        "  def __init__(self, eps, Q, action_space):\n",
        "    self.eps = eps\n",
        "    self.Q = Q\n",
        "    self.action_space = action_space\n",
        "\n",
        "  def get_action(self, state):\n",
        "    eps_val = np.random.uniform(0, 1)\n",
        "\n",
        "    if eps_val < self.eps:\n",
        "      return self.action_space.sample()\n",
        "    else:\n",
        "      return int(torch.argmax(self.Q(state)))\n",
        "        \n",
        "\n",
        "  def update_eps(self, eps):\n",
        "    self.eps = eps\n",
        "\n",
        "  def __str__(self):\n",
        "    return f'This agent works in an environment with {len(self.Q)} states and {self.action_space.n} possible actions'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLw7oStVak6K"
      },
      "source": [
        "## Actor-Critic: Acrobot Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fZuEd2Dak6L"
      },
      "outputs": [],
      "source": [
        "#DQN-Learning: Applied to Cart Pole\n",
        "\n",
        "#Initialize HyperParameters\n",
        "params = {\"copyInterval\": 100, \"lr\": 0.001, \"momentum\": 0.9, \"gamma\": 0.99, \"epsilon\" : 1, \"endEpsilon\": 0.01, \"maxReplay\": 256, \"miniBatchSize\": 10, \"numEpisodes\": 2000}\n",
        "\n",
        "\n",
        "#Initialize Environment\n",
        "env = gym.make('Acrobot-v1')\n",
        "\n",
        "#Create list for total rewards\n",
        "totalRewards = []\n",
        "\n",
        "#Create list for average of last ten rewards\n",
        "averages = []\n",
        "\n",
        "#Initialize Networks\n",
        "Q = Net()\n",
        "Q_Target = copy.deepcopy(Q)\n",
        "copyInterval = params[\"copyInterval\"]\n",
        "step = 0\n",
        "\n",
        "#Initialize Network (Actor)\n",
        "Actor = ActorNet()\n",
        "\n",
        "#Define the loss function\n",
        "criterion = nn.MSELoss()\n",
        "optimizerQ = optim.Adam(Q.parameters(), lr=0.001)\n",
        "\n",
        "#Define optimizer for actor\n",
        "optimizerA = optim.Adam(Actor.parameters(), lr = 0.000001)\n",
        "\n",
        "#Define Discount Factor (gamma)\n",
        "gamma = params[\"gamma\"]\n",
        "\n",
        "#Initialize Greedy Epsilon Agent for Q\n",
        "epsilon = params[\"epsilon\"]\n",
        "endEpsilon = params[\"endEpsilon\"]\n",
        "agent = GreedyEpsAgent(epsilon, Q, env.action_space)\n",
        "\n",
        "#Initialize agent for Actor\n",
        "AAgent = ActorAgent(Actor, env.action_space)\n",
        "\n",
        "#Define Experience Replay list, its max size, and the mini batch size\n",
        "replay = []\n",
        "maxReplay = params[\"maxReplay\"]\n",
        "miniBatchSize = params[\"miniBatchSize\"]\n",
        "\n",
        "#Our initial policy can be an even distribution\n",
        "numEpisodes = params[\"numEpisodes\"]\n",
        "episode = 0\n",
        "\n",
        "#Loop through episodes, training agent using Q-Learning\n",
        "while(episode < numEpisodes):\n",
        "  s = env.reset()\n",
        "  done = False\n",
        "\n",
        "  rTracker = 0\n",
        "  counter = 0\n",
        "  while not done:\n",
        "    step += 1\n",
        "    counter += 1\n",
        "\n",
        "    #Take action based on Actor\n",
        "    a = AAgent.get_action(torch.tensor(s).float())\n",
        "    sP, r, done, info = env.step(a)\n",
        "    r = np.clip(r, -1, 1)\n",
        "    #Take random minibatch for Actor and Critic\n",
        "    minibatch = random.sample(replay, min(len(replay), miniBatchSize))\n",
        "\n",
        "    if len(minibatch) > 2:\n",
        "      #Break up minibatch into the different columns of values\n",
        "      columns = list(zip(*minibatch))\n",
        "      states = torch.reshape(torch.tensor(columns[0]).float(),[len(minibatch),6])\n",
        "      actions = torch.tensor(columns[1])\n",
        "      rewards = torch.tensor(columns[2])\n",
        "      newStates = torch.reshape(torch.tensor(columns[3]).float(),[len(minibatch), 6])\n",
        "      dones = torch.tensor(columns[4])\n",
        "\n",
        "      #Get targets and outputs (critic)\n",
        "      targets = torch.mul(gamma*torch.max(Q_Target(newStates), 1)[0], dones) + rewards\n",
        "      outputs = torch.reshape(torch.gather(Q(states), 1, actions), [len(minibatch)]).float()\n",
        "      targets = targets.float()\n",
        "\n",
        "      #Get Outputs for Actor\n",
        "      outputsA = torch.log(torch.reshape(torch.gather(Actor(states), 1, actions), [len(minibatch)]))\n",
        "      o = torch.reshape(torch.gather(Q(states), 1, actions), [len(minibatch)])\n",
        "\n",
        "      #Calculate loss and gradients (actor)\n",
        "      optimizerA.zero_grad()\n",
        "      lossA = -1*torch.mean(torch.mul(outputsA, o))\n",
        "      lossA.backward()\n",
        "      optimizerA.step()\n",
        "\n",
        "      #Calculate loss and gradients (critic)\n",
        "      optimizerQ.zero_grad()\n",
        "      lossQ = criterion(outputs, targets).float()\n",
        "      lossQ.backward()\n",
        "      optimizerQ.step()\n",
        "\n",
        "\n",
        "    #Storing in replay\n",
        "    d = int(not done)\n",
        "    if len(replay) >= maxReplay:\n",
        "      replay.append([s,[a],r,sP, d])\n",
        "      replay.pop(0)\n",
        "    else:\n",
        "      replay.append([s,[a],r,sP, d])\n",
        "    s = sP\n",
        "    rTracker += r\n",
        "  #Updating target Q if necessary\n",
        "  if step > copyInterval:\n",
        "    step  = 0\n",
        "    Q_Target = copy.deepcopy(Q)\n",
        "  \n",
        "  #Increment Episode\n",
        "  episode += 1\n",
        "  #Append new Reward to totalRewards\n",
        "  totalRewards.append(rTracker)\n",
        "\n",
        "\n",
        "#Create and reset Deterministic Grid Environment\n",
        "env = gym.make('Acrobot-v1')\n",
        "obs = env.reset()\n",
        "\n",
        "#Take random steps in environment until done\n",
        "done = False\n",
        "while not done:\n",
        "  a = AAgent.get_action(torch.tensor(obs).float())\n",
        "  obs, reward, done, info = env.step(a)\n",
        "\n",
        "\n",
        "plt.plot([i for i in range(len(totalRewards))], totalRewards)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "#plt.plot([i for i in range(len(cumRewards))], cumRewards)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UWU05ToqYdL"
      },
      "source": [
        "# Traffic Junction 4 ðŸŽ® ðŸŽ® ðŸŽ® ðŸŽ®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96U1_Z5YqYdS"
      },
      "source": [
        "### Greedy Eps Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdkMKkYPqYdS"
      },
      "outputs": [],
      "source": [
        "#Agent that uses the greedy epsilon approach\n",
        "class GreedyEpsAgent2():\n",
        "\n",
        "  def __init__(self, eps, Q, action_space):\n",
        "    self.eps = eps\n",
        "    self.Q = Q\n",
        "    self.action_space = action_space\n",
        "\n",
        "  def get_action(self, state):\n",
        "    eps_val = np.random.uniform(0, 1)\n",
        "\n",
        "    if eps_val < self.eps:\n",
        "      return self.action_space.sample()[0]\n",
        "    else:\n",
        "      return int(torch.argmax(self.Q(state)))\n",
        "        \n",
        "\n",
        "  def update_eps(self, eps):\n",
        "    self.eps = eps\n",
        "\n",
        "  def __str__(self):\n",
        "    return f'This agent works in an environment with {len(self.Q)} states and {self.action_space.n} possible actions'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6TH6r1BqYdU"
      },
      "source": [
        "### NN Definition (Our Q-Approximation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONf7zyE-qYdV"
      },
      "outputs": [],
      "source": [
        "class Switch4Net(nn.Module):\n",
        "    def __init__(self, nAgents):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(nAgents*81, 100)\n",
        "        self.fc2 = nn.Linear(100, 50)\n",
        "        self.fc3 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOFl8-NRqYdV"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNx2E-bNqYdV"
      },
      "outputs": [],
      "source": [
        "#DQN-Learning: Applied to Deterministic Grid Environment\n",
        "\n",
        "#Initialize Environment\n",
        "env = gym.make('ma_gym:TrafficJunction4-v0')\n",
        "numAgents = 4\n",
        "\n",
        "#Create list for total rewards\n",
        "totalRewards = [[] for i in range(numAgents)]\n",
        "\n",
        "#Initialize Networks\n",
        "Qs = [Switch4Net(numAgents) for i in range(numAgents)]\n",
        "Q_Targets = [copy.deepcopy(Q) for Q in Qs]\n",
        "copyInterval = 500\n",
        "step = 0\n",
        "\n",
        "#Define the loss function\n",
        "criterion = nn.MSELoss()\n",
        "optimizers = [optim.Adam(Q.parameters(), lr=0.00001) for Q in Qs]\n",
        "\n",
        "#Define Discount Factor (gamma)\n",
        "gamma = 0.9\n",
        "\n",
        "#Initialize Greedy Epsilon Agent for Q\n",
        "epsilon = 1\n",
        "endEpsilon = 0.01\n",
        "agents = [GreedyEpsAgent2(epsilon, Q, env.action_space) for Q in Qs]\n",
        "\n",
        "#Define Experience Replay list, its max size, and the mini batch size\n",
        "replays = [[] for i in range(numAgents)]\n",
        "maxReplay = 512\n",
        "miniBatchSize = 52\n",
        "\n",
        "#Our initial policy can be an even distribution\n",
        "numEpisodes = 1000\n",
        "episode = 0\n",
        "\n",
        "#Loop through episodes, training agent using Q-Learning\n",
        "while(episode < numEpisodes):\n",
        "  s = env.reset()\n",
        "  s = [element for sublist in s for element in sublist]\n",
        "  done = [False for i in range(env.n_agents)]\n",
        "\n",
        "  rTracker = [0 for i in range(numAgents)]\n",
        "  while not all(done):\n",
        "    step += 1\n",
        "\n",
        "    #Take action based on greatest Q-Value from Q for state s\n",
        "    a = [agent.get_action(torch.tensor([[s]]).float()) for agent in agents]\n",
        "    sP, r, done, info = env.step(a)\n",
        "    sP = [element for sublist in sP for element in sublist]\n",
        "\n",
        "    #Zero out gradient and take random minibatch\n",
        "    for optimizer in optimizers:\n",
        "      optimizer.zero_grad()\n",
        "    \n",
        "    for i in range(numAgents):\n",
        "      minibatch = random.sample(replays[i], min(len(replays[i]), miniBatchSize))\n",
        "      if len(minibatch) > 3:\n",
        "        #Break up minibatch into the different columns of values\n",
        "        columns = list(zip(*minibatch))\n",
        "        states = torch.reshape(torch.tensor(columns[0]).float(),[len(minibatch), 81*numAgents])\n",
        "        actions = torch.tensor(columns[1])\n",
        "        rewards = torch.tensor(columns[2])\n",
        "        newStates = torch.reshape(torch.tensor(columns[3]).float(),[len(minibatch), 81*numAgents])\n",
        "\n",
        "        #Select Q and Q_Target\n",
        "        Q = Qs[i]\n",
        "        Q_Target = Q_Targets[i]\n",
        "\n",
        "        #Get targets and outputs\n",
        "        targets = (gamma*torch.max(Q_Target(newStates), 1)[0]) + rewards\n",
        "        outputs = torch.reshape(torch.gather(Q(states), 1, actions), [len(minibatch)])\n",
        "\n",
        "        #Calculate loss and gradients\n",
        "        loss = torch.mean(torch.abs(outputs - targets))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizers[i].step()\n",
        "\n",
        "    #Storing in replay\n",
        "    for i,replay in enumerate(replays):\n",
        "      if len(replay) >= maxReplay:\n",
        "        replay.append([s,[a[i]],r[i],sP,done[i]])\n",
        "        replay.pop(0)\n",
        "      else:\n",
        "        replay.append([s,[a[i]],r[i],sP,done[i]])\n",
        "\n",
        "    #Updating target Q if necessary\n",
        "    if step % copyInterval == 0:\n",
        "      Q_Targets = [copy.deepcopy(Q) for Q in Qs]\n",
        "\n",
        "    s = sP\n",
        "    for i in range(numAgents):\n",
        "      rTracker[i] = rTracker[i] + r[i]\n",
        "\n",
        "  episode += 1\n",
        "  for i in range(numAgents):\n",
        "    totalRewards[i].append(rTracker[i])\n",
        "\n",
        "  #Update epsilons for greedy-eps\n",
        "  for agent in agents:\n",
        "      agent.update_eps((endEpsilon/epsilon)**(episode/numEpisodes))\n",
        "\n",
        "\n",
        "rs = []\n",
        "for i in range(numEpisodes):\n",
        "  rTemp = 0\n",
        "  for j in range(len(totalRewards)):\n",
        "    rTemp += totalRewards[j][i]\n",
        "  rs.append(rTemp)\n",
        "\n",
        "plt.plot(rs)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.show()\n",
        "\n",
        "#Take random steps in environment until done\n",
        "for agent in agents:\n",
        "  agent.update_eps(0)\n",
        "s = env.reset()\n",
        "s = [element for sublist in s for element in sublist]\n",
        "done = [False for i in range(env.n_agents)]\n",
        "while not all(done):\n",
        "  a = [agent.get_action(torch.tensor([[s]]).float()) for agent in agents]\n",
        "  s, r, done, info = env.step(a)\n",
        "  s = [element for sublist in s for element in sublist]\n",
        "plt.imshow(env.render('rgb_array'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l1ggKbPBjl2"
      },
      "source": [
        "# Checkers ðŸŽ® ðŸŽ® ðŸŽ® ðŸŽ® (This one takes a while to train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkX6wEdIRaml"
      },
      "source": [
        "### Greedy Eps Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llm6co-1hSIR"
      },
      "outputs": [],
      "source": [
        "#Agent that uses the greedy epsilon approach\n",
        "class GreedyEpsAgent2():\n",
        "\n",
        "  def __init__(self, eps, Q, action_space):\n",
        "    self.eps = eps\n",
        "    self.Q = Q\n",
        "    self.action_space = action_space\n",
        "\n",
        "  def get_action(self, state):\n",
        "    eps_val = np.random.uniform(0, 1)\n",
        "\n",
        "    if eps_val < self.eps:\n",
        "      return self.action_space.sample()[0]\n",
        "    else:\n",
        "      return int(torch.argmax(self.Q(state)))\n",
        "        \n",
        "\n",
        "  def update_eps(self, eps):\n",
        "    self.eps = eps\n",
        "\n",
        "  def __str__(self):\n",
        "    return f'This agent works in an environment with {len(self.Q)} states and {self.action_space.n} possible actions'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7PX3gN1M9YG"
      },
      "source": [
        "### NN Definition (Our Q-Approximation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZI8bWuaM9YG"
      },
      "outputs": [],
      "source": [
        "class Switch4Net(nn.Module):\n",
        "    def __init__(self, nAgents):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(nAgents*47, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 500)\n",
        "        self.fc3 = nn.Linear(500, 250)\n",
        "        self.fc4 = nn.Linear(250, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79yLj1rvM6VK"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgb9Avvvd5-X"
      },
      "outputs": [],
      "source": [
        "#DQN-Learning: Applied to Deterministic Grid Environment\n",
        "\n",
        "#Initialize Environment\n",
        "env = gym.make('ma_gym:Checkers-v0')\n",
        "numAgents = 2\n",
        "\n",
        "#Create list for total rewards\n",
        "totalRewards = []\n",
        "\n",
        "#Initialize Networks\n",
        "Qs = [Switch4Net(numAgents) for i in range(numAgents)]\n",
        "Q_Targets = [copy.deepcopy(Q) for Q in Qs]\n",
        "copyInterval = 5000\n",
        "step = 0\n",
        "\n",
        "#Define the loss function\n",
        "criterion = nn.MSELoss()\n",
        "optimizers = [optim.Adam(Q.parameters()) for Q in Qs]\n",
        "\n",
        "#Define Discount Factor (gamma)\n",
        "gamma = 0.99\n",
        "\n",
        "#Initialize Greedy Epsilon Agent for Q\n",
        "epsilon = 1\n",
        "endEpsilon = 0.01\n",
        "agents = [GreedyEpsAgent2(epsilon, Q, env.action_space) for Q in Qs]\n",
        "\n",
        "#Define Experience Replay list, its max size, and the mini batch size\n",
        "replays = [[] for i in range(numAgents)]\n",
        "maxReplay = 2048\n",
        "miniBatchSize = 256\n",
        "\n",
        "#Our initial policy can be an even distribution\n",
        "numEpisodes = 1000\n",
        "episode = 0\n",
        "\n",
        "#Loop through episodes, training agent using Q-Learning\n",
        "while(episode < numEpisodes):\n",
        "  s = env.reset()\n",
        "  s = [element for sublist in s for element in sublist]\n",
        "  done = [False for i in range(env.n_agents)]\n",
        "\n",
        "  rTracker = 0\n",
        "  while not all(done):\n",
        "    step += 1\n",
        "\n",
        "    #Take action based on greatest Q-Value from Q for state s\n",
        "    a = [agent.get_action(torch.tensor([[s]]).float()) for agent in agents]\n",
        "    sP, r, done, info = env.step(a)\n",
        "    sP = [element for sublist in sP for element in sublist]\n",
        "\n",
        "    #Zero out gradient and take random minibatch\n",
        "    for optimizer in optimizers:\n",
        "      optimizer.zero_grad()\n",
        "    \n",
        "    for i in range(numAgents):\n",
        "      minibatch = random.sample(replays[i], min(len(replays[i]), miniBatchSize))\n",
        "      if len(minibatch) > 3:\n",
        "        #Break up minibatch into the different columns of values\n",
        "        columns = list(zip(*minibatch))\n",
        "        states = torch.reshape(torch.tensor(columns[0]).float(),[len(minibatch),47*numAgents])\n",
        "        actions = torch.tensor(columns[1])\n",
        "        rewards = torch.tensor(columns[2])\n",
        "        newStates = torch.reshape(torch.tensor(columns[3]).float(),[len(minibatch), 47*numAgents])\n",
        "\n",
        "        #Select Q and Q_Target\n",
        "        Q = Qs[i]\n",
        "        Q_Target = Q_Targets[i]\n",
        "\n",
        "        #Get targets and outputs\n",
        "        targets = (gamma*torch.max(Q_Target(newStates), 1)[0]) + rewards\n",
        "        outputs = torch.reshape(torch.gather(Q(states), 1, actions), [len(minibatch)])\n",
        "\n",
        "        #Calculate loss and gradients\n",
        "        loss = torch.mean(torch.abs(outputs - targets))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizers[i].step()\n",
        "\n",
        "    #Storing in replay\n",
        "    for i,replay in enumerate(replays):\n",
        "      if len(replay) >= maxReplay:\n",
        "        replay.append([s,[a[i]],r[i],sP,done[i]])\n",
        "        replay.pop(0)\n",
        "      else:\n",
        "        replay.append([s,[a[i]],r[i],sP,done[i]])\n",
        "\n",
        "    #Updating target Q if necessary\n",
        "    if step % copyInterval == 0:\n",
        "      Q_Targets = [copy.deepcopy(Q) for Q in Qs]\n",
        "\n",
        "    s = sP\n",
        "    rTracker += sum(r)\n",
        "  episode += 1\n",
        "  totalRewards.append(rTracker)\n",
        "  for agent in agents:\n",
        "      agent.update_eps((endEpsilon/epsilon)**(episode/numEpisodes))\n",
        "\n",
        "plt.plot([i for i in range(len(totalRewards))], totalRewards)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.show()\n",
        "\n",
        "#Take random steps in environment until done\n",
        "for agent in agents:\n",
        "  agent.update_eps(0)\n",
        "s = env.reset()\n",
        "s = [element for sublist in s for element in sublist]\n",
        "done = [False for i in range(env.n_agents)]\n",
        "while not all(done):\n",
        "  a = [agent.get_action(torch.tensor([[s]]).float()) for agent in agents]\n",
        "  s, r, done, info = env.step(a)\n",
        "  s = [element for sublist in s for element in sublist]\n",
        "plt.imshow(env.render('rgb_array'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bcc_DHQRc7f8",
        "U37mCZKidJ-X",
        "59nXVc85cmeo",
        "8_3hk5P0eMjJ",
        "ajtHQbGfkXrH"
      ],
      "name": "CSE676_final_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
